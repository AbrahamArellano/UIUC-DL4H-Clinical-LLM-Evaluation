{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "254c3a7a",
      "metadata": {
        "id": "254c3a7a"
      },
      "source": [
        "# üöÄ DLH Project Phase 3: Model Management"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4136a209",
      "metadata": {
        "id": "4136a209"
      },
      "source": [
        "\n",
        "## üìÅ Project Structure Setup\n",
        "\n",
        "This notebook assumes the following DL4H-Project directory structure in Google Drive:\n",
        "\n",
        "```\n",
        "/DL4H-Project\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ data/                  # Preprocessed datasets\n",
        "‚îú‚îÄ‚îÄ models/                # Pretrained and fine-tuned models\n",
        "‚îú‚îÄ‚îÄ results/               # Evaluation and metrics\n",
        "‚îú‚îÄ‚îÄ logs/                  # Training logs\n",
        "‚îî‚îÄ‚îÄ notebooks/             # Project notebooks\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "877291d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "877291d6",
        "outputId": "6a674abf-3b08-4e2c-e71a-82f7e41e54ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Project paths set up.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Mount Google Drive and set up paths\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/DL4H-Project\"\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
        "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
        "LOGS_DIR = os.path.join(BASE_DIR, \"logs\")\n",
        "\n",
        "print(\"‚úÖ Project paths set up.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9baf8f0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9baf8f0b",
        "outputId": "a0795579-1943-4f4b-b9ae-1f9f958d1be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "# !pip uninstall -y transformers tokenizers\n",
        "!pip install transformers tokenizers\n",
        "# !pip install tokenizers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3fe9cca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3fe9cca",
        "outputId": "857d4b4a-57f2-4589-c275-ffb09ca3b811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.51.3\n",
            "‚úÖ Libraries imported.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Import common libraries\n",
        "import torch\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "print(transformers.__version__)\n",
        "print(\"‚úÖ Libraries imported.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5057f28d",
      "metadata": {
        "id": "5057f28d"
      },
      "source": [
        "\n",
        "## üß† Phase 3.1: Model Registry and Configuration\n",
        "\n",
        "This step sets up a centralized model registry, unified loading logic, and config tracking system. It prepares both general-purpose and clinical models for fine-tuning and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "483c7567",
      "metadata": {
        "id": "483c7567"
      },
      "outputs": [],
      "source": [
        "# Model Registry and Configuration\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration, T5Tokenizer,\n",
        "    RobertaForSequenceClassification, RobertaTokenizer,\n",
        "    AutoModelForSequenceClassification, AutoTokenizer\n",
        ")\n",
        "\n",
        "# Model Registry\n",
        "MODEL_REGISTRY = {\n",
        "    \"t5-base\": {\n",
        "        \"version\": \"v1.0\",\n",
        "        \"model_class\": T5ForConditionalGeneration,\n",
        "        \"tokenizer_class\": T5Tokenizer,\n",
        "        \"pretrained\": \"t5-base\",\n",
        "        \"type\": \"general\",\n",
        "        \"size\": \"base\",\n",
        "        \"params\": 220_000_000,\n",
        "        \"task_head\": \"seq2seq\",\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"dropout\": 0.1,\n",
        "        \"notes\": \"Standard T5-Base for general tasks\"\n",
        "    },\n",
        "    \"roberta-large\": {\n",
        "        \"version\": \"v1.0\",\n",
        "        \"model_class\": RobertaForSequenceClassification,\n",
        "        \"tokenizer_class\": RobertaTokenizer,\n",
        "        \"pretrained\": \"roberta-large\",\n",
        "        \"type\": \"general\",\n",
        "        \"size\": \"large\",\n",
        "        \"params\": 355_000_000,\n",
        "        \"task_head\": \"classification\",\n",
        "        \"learning_rate\": 2e-5,\n",
        "        \"dropout\": 0.1,\n",
        "        \"notes\": \"General-purpose RoBERTa-Large model for classification tasks\"\n",
        "    },\n",
        "    \"bioclin_roberta\": {\n",
        "        \"version\": \"v1.0\",\n",
        "        \"model_class\": AutoModelForSequenceClassification,\n",
        "        \"tokenizer_class\": AutoTokenizer,\n",
        "        \"pretrained\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
        "        \"type\": \"clinical\",\n",
        "        \"size\": \"base\",\n",
        "        \"params\": 110_000_000,\n",
        "        \"task_head\": \"classification\",\n",
        "        \"learning_rate\": 2e-5,\n",
        "        \"dropout\": 0.1,\n",
        "        \"notes\": \"Clinical model built on Bio_ClinicalBERT\"\n",
        "    },\n",
        "    \"clinical_t5-base\": {\n",
        "        \"version\": \"v1.0\",\n",
        "        \"model_class\": T5ForConditionalGeneration,\n",
        "        \"tokenizer_class\": T5Tokenizer,\n",
        "        \"pretrained\": \"StanfordAIMI/clinical-t5-base\",\n",
        "        \"type\": \"clinical\",\n",
        "        \"size\": \"base\",\n",
        "        \"params\": 220_000_000,\n",
        "        \"task_head\": \"seq2seq\",\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"dropout\": 0.1,\n",
        "        \"notes\": \"Pretrained from scratch on MIMIC clinical notes\"\n",
        "    },\n",
        "    \"gatortron\": {\n",
        "        \"version\": \"v1.0\",\n",
        "        \"model_class\": None,  # Placeholder‚Äîrequires custom loading\n",
        "        \"tokenizer_class\": None,\n",
        "        \"pretrained\": \"Custom-GatorTron-Checkpoint\",\n",
        "        \"type\": \"clinical\",\n",
        "        \"size\": \"large\",\n",
        "        \"params\": 345_000_000,\n",
        "        \"task_head\": \"classification\",\n",
        "        \"learning_rate\": 2e-5,\n",
        "        \"dropout\": 0.1,\n",
        "        \"notes\": \"Custom configuration: use DeepSpeed/TPU environment as necessary\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Unified loader\n",
        "def load_model(model_key):\n",
        "    config = MODEL_REGISTRY[model_key]\n",
        "    if model_key == \"gatortron\":\n",
        "        # Enhanced placeholder for GatorTron\n",
        "        print(\"Loading GatorTron with custom configuration...\")\n",
        "        # Here you would include your custom loading logic.\n",
        "        # For demonstration, we'll simulate a loaded model.\n",
        "        model = \"GatorTron_model_object\"  # Replace with actual model loading\n",
        "        tokenizer = \"GatorTron_tokenizer_object\"  # Replace accordingly\n",
        "    else:\n",
        "        model = config[\"model_class\"].from_pretrained(config[\"pretrained\"])\n",
        "        tokenizer = config[\"tokenizer_class\"].from_pretrained(config[\"pretrained\"])\n",
        "    print(f\"‚úÖ Loaded {model_key} (version: {config['version']}) with ~{config['params']:,} parameters.\")\n",
        "    return model, tokenizer, config\n",
        "\n",
        "def log_model_config(model_key):\n",
        "    config = MODEL_REGISTRY[model_key]\n",
        "    print(f\"\\nüìù Model Configuration for {model_key}:\")\n",
        "    for k, v in config.items():\n",
        "        print(f\" - {k}: {v}\")\n",
        "\n",
        "# Example usage:\n",
        "# model, tokenizer, conf = load_model(\"t5-base\")\n",
        "# log_model_config(\"t5-base\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adapter Module for Task-Specific Customization\n",
        "This simple adapter function demonstrates how to attach a task-specific module (e.g., an extra linear layer)\n",
        "to the base model. This is useful for fine-tuning general models to clinical tasks."
      ],
      "metadata": {
        "id": "FNNHoKUU7y7a"
      },
      "id": "FNNHoKUU7y7a"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def add_adapter(model, input_dim, output_dim, adapter_name=\"adapter\"):\n",
        "    \"\"\"\n",
        "    Adds a simple feed-forward adapter to the given model. For illustration, the adapter is a linear layer.\n",
        "    In practice, you might want more complex modules (e.g., bottleneck adapters or LoRA-based modules).\n",
        "    \"\"\"\n",
        "    adapter = nn.Linear(input_dim, output_dim)\n",
        "    # Store the adapter in model's module dictionary\n",
        "    setattr(model, adapter_name, adapter)\n",
        "    print(f\"‚úÖ Adapter '{adapter_name}' added with input dim {input_dim} and output dim {output_dim}.\")\n",
        "    return model\n",
        "\n",
        "# Usage example (for a model returning hidden states of dimension 768):\n",
        "# model = add_adapter(model, 768, 768, adapter_name=\"task_adapter\")\n"
      ],
      "metadata": {
        "id": "P05ysh_x7aGQ"
      },
      "id": "P05ysh_x7aGQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèóÔ∏è Phase 3.2: General Domain Models Setup\n",
        "\n",
        "This step loads and configures T5 and RoBERTa models for limited-memory Colab environments, including:\n",
        "- üß† T5-Base\n",
        "- üöÄ T5-Large (with memory optimization)\n",
        "- üß± RoBERTa-Large (for classification tasks)"
      ],
      "metadata": {
        "id": "u16TFOfF5JdA"
      },
      "id": "u16TFOfF5JdA"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "import torch\n",
        "\n",
        "# T5-Base: Lightweight and Colab-friendly\n",
        "def load_t5_base():\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "    print(\"‚úÖ T5-Base loaded.\")\n",
        "    return model, tokenizer\n",
        "\n",
        "# T5-Large: Use with GPU memory optimization\n",
        "def load_t5_large():\n",
        "    from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
        "    from transformers import AutoConfig\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\"t5-large\")\n",
        "    with init_empty_weights():\n",
        "        model = T5ForConditionalGeneration(config)\n",
        "    model = load_checkpoint_and_dispatch(\n",
        "        model, \"t5-large\", device_map=\"auto\", no_split_module_classes=[\"T5Block\"]\n",
        "    )\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
        "    print(\"‚úÖ T5-Large loaded with memory-efficient config.\")\n",
        "    return model, tokenizer\n",
        "\n",
        "# RoBERTa-Large: For classification\n",
        "def load_roberta_large(num_labels=3):\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=num_labels)\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
        "    print(\"‚úÖ RoBERTa-Large loaded.\")\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "JkfrYcsO5LUA"
      },
      "id": "JkfrYcsO5LUA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ü©∫ Phase 3.3: Clinical Domain Models Setup\n",
        "\n",
        "This step sets up domain-specific language models tailored for clinical data, including:\n",
        "- üè• BioClinicalBERT (BioClinRoBERTa)\n",
        "- üß† Clinical-T5 (base variant)\n",
        "- ‚öôÔ∏è GatorTron setup placeholder (for specialist deployment) -- ***GatorTron presents significant reproducibility challenges as it was trained on proprietary University of Florida Health data and deliberately not publicly released due to patient privacy concerns. The paper explicitly acknowledges this limitation, noting clinical models may retain sensitive health information. For this reproduction, BioClinRoBERTa (already implemented in the notebook) serves as an appropriate alternative, as it's also a specialized clinical model with comparable architecture (345M parameters) and demonstrated similar performance in the original paper. This substitution allows us to maintain scientific validity while respecting data access constraints.***\n",
        "\n",
        "It ensures tokenizer compatibility and introduces checkpoint handling strategies.\n"
      ],
      "metadata": {
        "id": "a_YkkXbf6S2-"
      },
      "id": "a_YkkXbf6S2-"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load BioClinicalBERT\n",
        "def load_bioclin_roberta(num_labels=3):\n",
        "    model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"‚úÖ BioClinicalBERT loaded.\")\n",
        "    return model, tokenizer\n",
        "\n",
        "# Load Clinical-T5 Base\n",
        "def load_clinical_t5_base():\n",
        "    model_name = \"StanfordAIMI/clinical-t5-base\"\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    print(\"‚úÖ Clinical-T5 Base loaded.\")\n",
        "    return model, tokenizer\n",
        "\n",
        "# Placeholder for GatorTron setup\n",
        "def configure_gatortron():\n",
        "    print(\"‚ö†Ô∏è GatorTron is a large model not hosted on HuggingFace. Use custom checkpoint management and TPU/DeepSpeed if available.\")\n",
        "\n",
        "#load_clinical_t5_base()"
      ],
      "metadata": {
        "id": "vRDI08wv6VMy"
      },
      "id": "vRDI08wv6VMy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üî¨ Phase 3.4: Model Verification and Analysis\n",
        "\n",
        "This step adds utilities to:\n",
        "- ‚úÖ Count model parameters and report size\n",
        "- ‚úÖ Validate model output formats\n",
        "- ‚úÖ Compare memory usage\n",
        "- ‚úÖ Benchmark inference time\n",
        "- üìù Document model configuration choices\n"
      ],
      "metadata": {
        "id": "gGXNvYTt8M7q"
      },
      "id": "gGXNvYTt8M7q"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "# Parameter counting\n",
        "def count_parameters(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"üì¶ Total parameters: {total:,}\")\n",
        "    print(f\"üß† Trainable parameters: {trainable:,}\")\n",
        "    return total, trainable\n",
        "\n",
        "# Basic output shape check\n",
        "def validate_model_output(model, tokenizer, task_type=\"seq2seq\", input_text=\"Translate English to French: Hello world\"):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "        if task_type == \"seq2seq\":\n",
        "            output = model.generate(**inputs)\n",
        "            decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "            print(f\"‚úÖ Seq2Seq Output: {decoded}\")\n",
        "        elif task_type == \"classification\":\n",
        "            inputs[\"labels\"] = torch.tensor([1])\n",
        "            output = model(**inputs)\n",
        "            print(f\"‚úÖ Classification logits shape: {output.logits.shape}\")\n",
        "\n",
        "# Inference time benchmarking\n",
        "def benchmark_model_inference(model, tokenizer, task_type=\"seq2seq\", input_text=\"Translate English to French: Hello world\", runs=5):\n",
        "    model.eval()\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(runs):\n",
        "            inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "            start = time.time()\n",
        "            if task_type == \"seq2seq\":\n",
        "                _ = model.generate(**inputs)\n",
        "            elif task_type == \"classification\":\n",
        "                inputs[\"labels\"] = torch.tensor([1])\n",
        "                _ = model(**inputs)\n",
        "            times.append(time.time() - start)\n",
        "    avg_time = sum(times) / runs\n",
        "    print(f\"‚è±Ô∏è Avg inference time over {runs} runs: {avg_time:.4f} seconds\")\n",
        "    return avg_time\n",
        "\n",
        "# Memory usage (approximate using torch.cuda.memory_allocated if on GPU)\n",
        "def report_memory_usage():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        used = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "        print(f\"üßÆ GPU memory used: {used:.2f} MB\")\n",
        "    else:\n",
        "        print(\"üíª Running on CPU. Use torch.cuda for GPU memory stats.\")\n",
        "\n",
        "# Config logger (documentation helper)\n",
        "def log_model_config(model_key, config=None):\n",
        "    if config is None:\n",
        "        config = MODEL_REGISTRY[model_key]\n",
        "    print(f\"\\nüìù Model Configuration for {model_key}:\")\n",
        "    for k, v in config.items():\n",
        "        print(f\" - {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "3NwIuDRS8QC6"
      },
      "id": "3NwIuDRS8QC6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Rigorous Model Validation Tests\n",
        "\n",
        "Below is a simple unit-test function that checks if a model, after loading, produces outputs of the expected shape.\n",
        "It also logs current GPU memory usage. You can extend this to include more detailed tests.\n"
      ],
      "metadata": {
        "id": "JBhTDLtB8XZ9"
      },
      "id": "JBhTDLtB8XZ9"
    },
    {
      "cell_type": "code",
      "source": [
        "def check_model_output(model, tokenizer, example_text=\"Translate English to French: Hello world\", expected_output_shape=(1,)):\n",
        "    \"\"\"\n",
        "    Tests the loaded model's output by performing a forward pass and printing the output shape.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    inputs = tokenizer(example_text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        # For generative models, we use generate; for classification, we use forward pass.\n",
        "        if hasattr(model, \"generate\"):\n",
        "            outputs = model.generate(**inputs)\n",
        "        else:\n",
        "            outputs = model(**inputs)\n",
        "    if isinstance(outputs, (list, tuple)):\n",
        "        output_tensor = outputs[0] if isinstance(outputs[0], torch.Tensor) else None\n",
        "    elif isinstance(outputs, torch.Tensor):\n",
        "        output_tensor = outputs\n",
        "    else:\n",
        "        output_tensor = None\n",
        "\n",
        "    if output_tensor is not None:\n",
        "        print(\"‚úÖ Model output shape:\", output_tensor.shape)\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Unable to determine output tensor shape.\")\n",
        "\n",
        "    # GPU Memory usage monitoring\n",
        "    if torch.cuda.is_available():\n",
        "        usage_mb = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
        "        print(f\"üßÆ GPU Memory Allocated: {usage_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(\"Running on CPU.\")\n",
        "\n",
        "# Example test:\n",
        "model, tokenizer, _ = load_model(\"t5-base\")\n",
        "check_model_output(model, tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOljAkWK8hOq",
        "outputId": "551f5932-6073-4353-db67-8b910076ae88"
      },
      "id": "gOljAkWK8hOq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded t5-base (version: v1.0) with ~220,000,000 parameters.\n",
            "‚úÖ Model output shape: torch.Size([1, 5])\n",
            "Running on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ Phase 3: Run Model Management for All Registered Models\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Example prompts based on model task type\n",
        "example_prompts = {\n",
        "    \"seq2seq\": \"Translate English to French: The patient was discharged today.\",\n",
        "    \"classification\": \"Patient has no signs of acute infection.\"\n",
        "}\n",
        "\n",
        "# Store results for comparison\n",
        "phase3_eval = []\n",
        "\n",
        "for model_name, meta in MODEL_REGISTRY.items():\n",
        "    print(f\"\\n\\n========================\")\n",
        "    print(f\"üîç Running Phase 3 for {model_name}\")\n",
        "    print(f\"========================\")\n",
        "\n",
        "    # Load model, tokenizer, config\n",
        "    try:\n",
        "        model, tokenizer, config = load_model(model_name)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {model_name}: {e}\")\n",
        "        continue\n",
        "    # Skip GatorTron or any placeholder models\n",
        "    if isinstance(model, str):\n",
        "      print(f\"‚ö†Ô∏è Skipping Phase 3 checks for {model_name} (placeholder)\")\n",
        "      continue\n",
        "\n",
        "    # Log configuration\n",
        "    log_model_config(model_name, config)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params, trainable_params = count_parameters(model)\n",
        "\n",
        "    # Select example input\n",
        "    input_text = example_prompts.get(config[\"task_head\"], example_prompts[\"classification\"])\n",
        "\n",
        "    # Validate model forward pass or generation\n",
        "    try:\n",
        "        validate_model_output(model, tokenizer, task_type=config[\"task_head\"], input_text=input_text)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Validation failed for {model_name}: {e}\")\n",
        "\n",
        "    # Benchmark inference time\n",
        "    try:\n",
        "        inf_time = benchmark_model_inference(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            task_type=config[\"task_head\"],\n",
        "            input_text=input_text,\n",
        "            runs=3\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Benchmarking failed for {model_name}: {e}\")\n",
        "        inf_time = None\n",
        "\n",
        "    # Memory report\n",
        "    try:\n",
        "        report_memory_usage()\n",
        "    except:\n",
        "        print(\"‚ÑπÔ∏è Skipping memory reporting.\")\n",
        "\n",
        "    # Save summary\n",
        "    phase3_eval.append({\n",
        "        \"model\": model_name,\n",
        "        \"task_type\": config[\"task_head\"],\n",
        "        \"params_total\": total_params,\n",
        "        \"params_trainable\": trainable_params,\n",
        "        \"inference_time_avg\": inf_time,\n",
        "        \"architecture\": config.get(\"architecture\", \"Unknown\"),\n",
        "        \"domain\": config.get(\"type\", \"Unknown\")\n",
        "    })\n",
        "\n",
        "# Summary table\n",
        "df_phase3 = pd.DataFrame(phase3_eval).set_index(\"model\")\n",
        "print(\"\\n‚úÖ Phase 3 Evaluation Summary:\")\n",
        "display(df_phase3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "84r-b1YRvlsx",
        "outputId": "318c17f0-0bc5-4034-d3ea-110895600684"
      },
      "id": "84r-b1YRvlsx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "========================\n",
            "üîç Running Phase 3 for t5-base\n",
            "========================\n",
            "‚úÖ Loaded t5-base (version: v1.0) with ~220,000,000 parameters.\n",
            "\n",
            "üìù Model Configuration for t5-base:\n",
            " - version: v1.0\n",
            " - model_class: <class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n",
            " - tokenizer_class: <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>\n",
            " - pretrained: t5-base\n",
            " - type: general\n",
            " - size: base\n",
            " - params: 220000000\n",
            " - task_head: seq2seq\n",
            " - learning_rate: 0.0001\n",
            " - dropout: 0.1\n",
            " - notes: Standard T5-Base for general tasks\n",
            "üì¶ Total parameters: 222,903,552\n",
            "üß† Trainable parameters: 222,903,552\n",
            "‚úÖ Seq2Seq Output: Le patient a √©t√© lib√©r√© aujourd'hui.\n",
            "‚è±Ô∏è Avg inference time over 3 runs: 0.8641 seconds\n",
            "üíª Running on CPU. Use torch.cuda for GPU memory stats.\n",
            "\n",
            "\n",
            "========================\n",
            "üîç Running Phase 3 for roberta-large\n",
            "========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded roberta-large (version: v1.0) with ~355,000,000 parameters.\n",
            "\n",
            "üìù Model Configuration for roberta-large:\n",
            " - version: v1.0\n",
            " - model_class: <class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>\n",
            " - tokenizer_class: <class 'transformers.models.roberta.tokenization_roberta.RobertaTokenizer'>\n",
            " - pretrained: roberta-large\n",
            " - type: general\n",
            " - size: large\n",
            " - params: 355000000\n",
            " - task_head: classification\n",
            " - learning_rate: 2e-05\n",
            " - dropout: 0.1\n",
            " - notes: General-purpose RoBERTa-Large model for classification tasks\n",
            "üì¶ Total parameters: 355,361,794\n",
            "üß† Trainable parameters: 355,361,794\n",
            "‚úÖ Classification logits shape: torch.Size([1, 2])\n",
            "‚è±Ô∏è Avg inference time over 3 runs: 0.3287 seconds\n",
            "üíª Running on CPU. Use torch.cuda for GPU memory stats.\n",
            "\n",
            "\n",
            "========================\n",
            "üîç Running Phase 3 for bioclin_roberta\n",
            "========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded bioclin_roberta (version: v1.0) with ~110,000,000 parameters.\n",
            "\n",
            "üìù Model Configuration for bioclin_roberta:\n",
            " - version: v1.0\n",
            " - model_class: <class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>\n",
            " - tokenizer_class: <class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>\n",
            " - pretrained: emilyalsentzer/Bio_ClinicalBERT\n",
            " - type: clinical\n",
            " - size: base\n",
            " - params: 110000000\n",
            " - task_head: classification\n",
            " - learning_rate: 2e-05\n",
            " - dropout: 0.1\n",
            " - notes: Clinical model built on Bio_ClinicalBERT\n",
            "üì¶ Total parameters: 108,311,810\n",
            "üß† Trainable parameters: 108,311,810\n",
            "‚úÖ Classification logits shape: torch.Size([1, 2])\n",
            "‚è±Ô∏è Avg inference time over 3 runs: 0.0940 seconds\n",
            "üíª Running on CPU. Use torch.cuda for GPU memory stats.\n",
            "\n",
            "\n",
            "========================\n",
            "üîç Running Phase 3 for clinical_t5-base\n",
            "========================\n",
            "‚ùå Error loading clinical_t5-base: StanfordAIMI/clinical-t5-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "\n",
            "\n",
            "========================\n",
            "üîç Running Phase 3 for gatortron\n",
            "========================\n",
            "Loading GatorTron with custom configuration...\n",
            "‚úÖ Loaded gatortron (version: v1.0) with ~345,000,000 parameters.\n",
            "‚ö†Ô∏è Skipping Phase 3 checks for gatortron (placeholder)\n",
            "\n",
            "‚úÖ Phase 3 Evaluation Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                      task_type  params_total  params_trainable  \\\n",
              "model                                                             \n",
              "t5-base                 seq2seq     222903552         222903552   \n",
              "roberta-large    classification     355361794         355361794   \n",
              "bioclin_roberta  classification     108311810         108311810   \n",
              "\n",
              "                 inference_time_avg architecture    domain  \n",
              "model                                                       \n",
              "t5-base                    0.864093      Unknown   general  \n",
              "roberta-large              0.328741      Unknown   general  \n",
              "bioclin_roberta            0.093977      Unknown  clinical  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c7a4e7cb-5eac-4481-ae0b-20b717d0f737\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>task_type</th>\n",
              "      <th>params_total</th>\n",
              "      <th>params_trainable</th>\n",
              "      <th>inference_time_avg</th>\n",
              "      <th>architecture</th>\n",
              "      <th>domain</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>t5-base</th>\n",
              "      <td>seq2seq</td>\n",
              "      <td>222903552</td>\n",
              "      <td>222903552</td>\n",
              "      <td>0.864093</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>general</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large</th>\n",
              "      <td>classification</td>\n",
              "      <td>355361794</td>\n",
              "      <td>355361794</td>\n",
              "      <td>0.328741</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>general</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bioclin_roberta</th>\n",
              "      <td>classification</td>\n",
              "      <td>108311810</td>\n",
              "      <td>108311810</td>\n",
              "      <td>0.093977</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>clinical</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7a4e7cb-5eac-4481-ae0b-20b717d0f737')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c7a4e7cb-5eac-4481-ae0b-20b717d0f737 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c7a4e7cb-5eac-4481-ae0b-20b717d0f737');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-79c2d62b-f780-4d73-9310-ec8a493494d6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-79c2d62b-f780-4d73-9310-ec8a493494d6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-79c2d62b-f780-4d73-9310-ec8a493494d6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_869a4969-b349-4a86-ae5d-1689ac186c97\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_phase3')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_869a4969-b349-4a86-ae5d-1689ac186c97 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_phase3');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_phase3",
              "summary": "{\n  \"name\": \"df_phase3\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"t5-base\",\n          \"roberta-large\",\n          \"bioclin_roberta\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"task_type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"classification\",\n          \"seq2seq\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"params_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 123632619,\n        \"min\": 108311810,\n        \"max\": 355361794,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          222903552,\n          355361794\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"params_trainable\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 123632619,\n        \"min\": 108311810,\n        \"max\": 355361794,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          222903552,\n          355361794\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inference_time_avg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.39471394088037726,\n        \"min\": 0.09397729237874348,\n        \"max\": 0.8640933036804199,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.8640933036804199,\n          0.32874131202697754\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"architecture\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Unknown\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"domain\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"clinical\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AC4hADjE3fWb"
      },
      "id": "AC4hADjE3fWb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}